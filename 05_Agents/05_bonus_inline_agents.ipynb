{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lYGwGM97sQ4"
      },
      "source": [
        "## Building Dynamic AI Assistants with Amazon Bedrock Inline Agents\n",
        "\n",
        "Before we dive into creating and using persistent agents, we'll walk through the process of setting up and invoking an inline agent, showcasing its flexibility and power in creating dynamic AI assistants. By following our progressive approach, you will gain a comprehensive understanding of how to use inline agents for various use cases and complexity levels. Throughout a single interactive conversation, we will demonstrate how the agent can be enhanced `on the fly` with new tools and instructions while maintaining context of our ongoing discussion.\n",
        "\n",
        "We'll build a simple Inline Agent with a code interpreter.\n",
        "\n",
        "## What are Inline Agents?\n",
        "\n",
        "[Inline agents](https://docs.aws.amazon.com/bedrock/latest/userguide/agents-create-inline.html) are a powerful feature of Amazon Bedrock that allow developers to create flexible and adaptable AI assistants.\n",
        "\n",
        "Unlike traditional static agents, inline agents can be dynamically configured at runtime, enabling real time adjustments to their behavior, capabilities, and knowledge base.\n",
        "\n",
        "Key features of inline agents include:\n",
        "\n",
        "1. **Dynamic configuration**: Modify the agent's instructions, action groups, and other parameters on the fly.\n",
        "2. **Flexible integration**: Easily incorporate external APIs and services as needed for each interaction.\n",
        "3. **Contextual adaptation**: Adjust the agent's responses based on user roles, preferences, or specific scenarios.\n",
        "\n",
        "## Why Use Inline Agents?\n",
        "\n",
        "Inline agents offer several advantages for building AI applications:\n",
        "\n",
        "1. **Rapid prototyping**: Quickly experiment with different configurations without redeploying your application.\n",
        "2. **Personalization**: Tailor the agent's capabilities to individual users or use cases in real time.\n",
        "3. **Scalability**: Efficiently manage a single agent that can adapt to multiple roles or functions.\n",
        "4. **Cost effectiveness**: Optimize resource usage by dynamically selecting only the necessary tools and knowledge for each interaction.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before you begin, make sure that you have:\n",
        "\n",
        "1. An active AWS account with access to Amazon Bedrock.\n",
        "2. Necessary permissions to create and invoke inline agents.\n",
        "3. Be sure to complete additonal prerequisites, visit [Amazon Bedrock Inline Agent prerequisites documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/inline-agent-prereq.html) to learn more.\n",
        "\n",
        "### Installing prerequisites\n",
        "Let's begin with installing the required packages. This step is important as you need `boto3` version `1.35.68` or later to use inline agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": [],
        "collapsed": true,
        "id": "4MDzL25I7sQ9",
        "outputId": "c9644db6-161a-46c4-fc32-7453861da78b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.38.32-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting botocore\n",
            "  Downloading botocore-1.38.32-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3)\n",
            "  Downloading s3transfer-0.13.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore) (2.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore) (1.17.0)\n",
            "Downloading boto3-1.38.32-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.38.32-py3-none-any.whl (13.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.13.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.38.32 botocore-1.38.32 jmespath-1.0.1 s3transfer-0.13.0\n"
          ]
        }
      ],
      "source": [
        "# uncomment to install the required python packages\n",
        "%pip install --upgrade boto3 botocore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "tags": [],
        "id": "plT5UNuf7sRA",
        "outputId": "7e03bc7f-43de-4130-e201-61d933ac4edb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script>Jupyter.notebook.kernel.restart()</script>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# # restart kernel\n",
        "from IPython.core.display import HTML\n",
        "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env AWS_ACCESS_KEY_ID=ASIA5G2VGGAXSEUCKOO3\n",
        "%env AWS_SECRET_ACCESS_KEY=RXIrd7rxc/kB5TxsW1Hk3oiOOgkClbO99Ktucubq\n",
        "%env AWS_SESSION_TOKEN=IQoJb3JpZ2luX2VjEKD//////////wEaCXVzLWVhc3QtMSJHMEUCIQCN/s1FDsoKYZzUobufZPEdrdkIjQXIoouTRZyJTWWPpQIgQzly84cMSD9VPvpFkH54XfmRw47e5zhe3OH4tNSh8w4qmAMIeBAAGgw5MDgwMjczNzU2NjMiDPPDCBeHk3hms0TF7Cr1Am249dMGkZtepGt9MGmJmwtLf1sMtFJM8jRQU6RhRYkLlIUa9PJ/g1XafzOK2GAaKa5GY3DrQrR9Bm6Vb8lA9a9fdt9mAhpzjWrZ6/5VLWnTYKUcC9D58lE013DMcyc3hnGcEeJ+i/ESCkJ81rLx5EqXFzm7NwP2VubqEA22CpXNb8FHNtNeJlKSS15HaKVW42+e1YMyzZAr89mQLKiQGpqZbJNDQat318CZZUPTgruD9sNg6cGs98vvnGSTQnTIZZDIWmAwocG3wsVfJmvKQsJhJjltyQTWX4ES6sXoFHGNtxWnjQpAkhLWz9DWy6xfj/G0rsM76fTJvByxSQSLgkD8CnIhxJ4MbjH9bc2uiEV8C2IuVF2/aYY1Elz1yWj0y6hNmsLX2A4PajhwzY7OC3SSxMnzUVYk9cxKKHr8DATw6uBxGvLEpL+pG2rZJj9FjpDQZKhKgV2o91zXxt5+rvySE9IBCHM3ha1jtTdtIwaJAz9DuQIws62RwgY6pgECuOBNLcMSY5eQGWcv39API3fg/vU3ZzXj5Yf0auggrV6FrJrxtSaj5ASnHDiweahAApM6GF77J5bjmF6n7y6hqlOoM/uovyWa/vaJdprHQCnb910VXuEd4FTaWCs4ooGAgPAEtx4465kJuIZY1zRrAP/+XFM3joOI8RCsHBZTvTgh+I2ZixkeFj8Y3fMGlTf8pxq00lDxeVW6pDceDqx+NAa+cstx"
      ],
      "metadata": {
        "id": "pqIcFxCB8vld",
        "outputId": "11602a17-e173-4b07-dbdf-02a85ac3341d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: AWS_ACCESS_KEY_ID=ASIA5G2VGGAXSEUCKOO3\n",
            "env: AWS_SECRET_ACCESS_KEY=RXIrd7rxc/kB5TxsW1Hk3oiOOgkClbO99Ktucubq\n",
            "env: AWS_SESSION_TOKEN=IQoJb3JpZ2luX2VjEKD//////////wEaCXVzLWVhc3QtMSJHMEUCIQCN/s1FDsoKYZzUobufZPEdrdkIjQXIoouTRZyJTWWPpQIgQzly84cMSD9VPvpFkH54XfmRw47e5zhe3OH4tNSh8w4qmAMIeBAAGgw5MDgwMjczNzU2NjMiDPPDCBeHk3hms0TF7Cr1Am249dMGkZtepGt9MGmJmwtLf1sMtFJM8jRQU6RhRYkLlIUa9PJ/g1XafzOK2GAaKa5GY3DrQrR9Bm6Vb8lA9a9fdt9mAhpzjWrZ6/5VLWnTYKUcC9D58lE013DMcyc3hnGcEeJ+i/ESCkJ81rLx5EqXFzm7NwP2VubqEA22CpXNb8FHNtNeJlKSS15HaKVW42+e1YMyzZAr89mQLKiQGpqZbJNDQat318CZZUPTgruD9sNg6cGs98vvnGSTQnTIZZDIWmAwocG3wsVfJmvKQsJhJjltyQTWX4ES6sXoFHGNtxWnjQpAkhLWz9DWy6xfj/G0rsM76fTJvByxSQSLgkD8CnIhxJ4MbjH9bc2uiEV8C2IuVF2/aYY1Elz1yWj0y6hNmsLX2A4PajhwzY7OC3SSxMnzUVYk9cxKKHr8DATw6uBxGvLEpL+pG2rZJj9FjpDQZKhKgV2o91zXxt5+rvySE9IBCHM3ha1jtTdtIwaJAz9DuQIws62RwgY6pgECuOBNLcMSY5eQGWcv39API3fg/vU3ZzXj5Yf0auggrV6FrJrxtSaj5ASnHDiweahAApM6GF77J5bjmF6n7y6hqlOoM/uovyWa/vaJdprHQCnb910VXuEd4FTaWCs4ooGAgPAEtx4465kJuIZY1zRrAP/+XFM3joOI8RCsHBZTvTgh+I2ZixkeFj8Y3fMGlTf8pxq00lDxeVW6pDceDqx+NAa+cstx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAWc6Gup7sRB"
      },
      "source": [
        "## Setup and Imports\n",
        "\n",
        "First, let's import the necessary libraries and set up our Bedrock client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "tags": [],
        "id": "a1ghQaRg7sRB"
      },
      "outputs": [],
      "source": [
        "from posix import environ\n",
        "import os\n",
        "import json\n",
        "from pprint import pprint\n",
        "import boto3\n",
        "from datetime import datetime\n",
        "import random\n",
        "import pprint\n",
        "from termcolor import colored\n",
        "from rich.console import Console\n",
        "from rich.markdown import Markdown\n",
        "\n",
        "AWS_ACCESS_KEY_ID = os.environ.get('AWS_ACCESS_KEY_ID')\n",
        "AWS_SECRET_ACCESS_KEY = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
        "AWS_SESSION_TOKEN = os.environ.get('AWS_SESSION_TOKEN')\n",
        "\n",
        "session = boto3.session.Session(aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "                                aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
        "                                region_name='us-east-1')\n",
        "region = session.region_name\n",
        "\n",
        "# Runtime Endpoints\n",
        "bedrock_rt_client = boto3.client(\n",
        "    \"bedrock-agent-runtime\",\n",
        "    region_name=region\n",
        ")\n",
        "\n",
        "sts_client = boto3.client(\"sts\")\n",
        "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
        "\n",
        "# To manage session id:\n",
        "random_int = random.randint(1,100000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peYmr_Vw7sRC"
      },
      "source": [
        "## Configuring the Inline Agent\n",
        "\n",
        "Next, we'll set up the basic configuration for our Amazon Bedrock Inline Agent. This includes specifying the foundation model, session management, and basic instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "tags": [],
        "id": "wGw6EU987sRC"
      },
      "outputs": [],
      "source": [
        "# change model id as needed:\n",
        "model_id = \"arn:aws:bedrock:us-east-1:908027375663:inference-profile/us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
        "\n",
        "sessionId = f'custom-session-id-{random_int}'\n",
        "endSession = False\n",
        "enableTrace = True\n",
        "\n",
        "# customize instructions of inline agent:\n",
        "agent_instruction = \"\"\"You are a helpful AI assistant helping Octank Inc employees with their questions and processes.\n",
        "You write short and direct responses while being cheerful. You have access to python coding environment that helps you extend your capabilities.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAWtOeMn7sRD"
      },
      "source": [
        "## Basic Inline Agent Invocation\n",
        "\n",
        "Let's start by invoking a simple inline agent with just the foundation model and basic instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "tags": [],
        "id": "-nt9YQhS7sRD"
      },
      "outputs": [],
      "source": [
        "# prepare request parameters before invoking inline agent\n",
        "request_params = {\n",
        "    \"instruction\": agent_instruction,\n",
        "    \"foundationModel\": model_id,\n",
        "    \"sessionId\": sessionId,\n",
        "    \"endSession\": endSession,\n",
        "    \"enableTrace\": enableTrace,\n",
        "}\n",
        "\n",
        "# define code interpreter tool\n",
        "code_interpreter_tool = {\n",
        "    \"actionGroupName\": \"UserInputAction\",\n",
        "    \"parentActionGroupSignature\": \"AMAZON.CodeInterpreter\"\n",
        "}\n",
        "\n",
        "# add the tool to request parameter of inline agent\n",
        "request_params[\"actionGroups\"] = [code_interpreter_tool]\n",
        "\n",
        "# enable traces\n",
        "request_params[\"enableTrace\"] = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "tags": [],
        "id": "o9AXcbMy7sRE"
      },
      "outputs": [],
      "source": [
        "# enter the question you want the inline agent to answer\n",
        "request_params['inputText'] = 'what is the time right now in pacific timezone?'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHFvsWMH7sRE"
      },
      "source": [
        "### Invoking a simple Inline Agent\n",
        "\n",
        "We'll send a request to the agent asking it to perform a simple calculation or code execution task. This will showcase how the agent can interpret and run code on the fly.\n",
        "\n",
        "To do so, we will use the [InvokeInlineAgent](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent-runtime_InvokeInlineAgent.html) API via boto3 `bedrock-agent-runtime` client.\n",
        "\n",
        "Our function `invoke_inline_agent_helper` also helps us processing the agent trace request and format it for easier readibility. You do not have to use this function in your system, but it will make it easier to observe the code used by code interpreter, the function invocations and the knowledge base content.\n",
        "\n",
        "We also provide the metrics for the agent invocation time and the input and output tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "tags": [],
        "id": "Ix59evNO7sRF"
      },
      "outputs": [],
      "source": [
        "def invoke_inline_agent_helper(client, request_params, trace_level=\"core\"):\n",
        "    _time_before_call = datetime.now()\n",
        "\n",
        "    _agent_resp = client.invoke_inline_agent(\n",
        "        **request_params\n",
        "    )\n",
        "\n",
        "    if request_params[\"enableTrace\"]:\n",
        "        if trace_level == \"all\":\n",
        "            print(f\"invokeAgent API response object: {_agent_resp}\")\n",
        "        else:\n",
        "            print(\n",
        "                f\"invokeAgent API request ID: {_agent_resp['ResponseMetadata']['RequestId']}\"\n",
        "            )\n",
        "            session_id = request_params[\"sessionId\"]\n",
        "            print(f\"invokeAgent API session ID: {session_id}\")\n",
        "\n",
        "    # Return error message if invoke was unsuccessful\n",
        "    if _agent_resp[\"ResponseMetadata\"][\"HTTPStatusCode\"] != 200:\n",
        "        _error_message = f\"API Response was not 200: {_agent_resp}\"\n",
        "        if request_params[\"enableTrace\"] and trace_level == \"all\":\n",
        "            print(_error_message)\n",
        "        return _error_message\n",
        "\n",
        "    _total_in_tokens = 0\n",
        "    _total_out_tokens = 0\n",
        "    _total_llm_calls = 0\n",
        "    _orch_step = 0\n",
        "    _sub_step = 0\n",
        "    _trace_truncation_lenght = 300\n",
        "    _time_before_orchestration = datetime.now()\n",
        "\n",
        "    _agent_answer = \"\"\n",
        "    _event_stream = _agent_resp[\"completion\"]\n",
        "\n",
        "    try:\n",
        "        for _event in _event_stream:\n",
        "            _sub_agent_alias_id = None\n",
        "\n",
        "            if \"chunk\" in _event:\n",
        "                _data = _event[\"chunk\"][\"bytes\"]\n",
        "                _agent_answer = _data.decode(\"utf8\")\n",
        "\n",
        "            if \"trace\" in _event and request_params[\"enableTrace\"]:\n",
        "                if \"failureTrace\" in _event[\"trace\"][\"trace\"]:\n",
        "                    print(\n",
        "                        colored(\n",
        "                            f\"Agent error: {_event['trace']['trace']['failureTrace']['failureReason']}\",\n",
        "                            \"red\",\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                if \"orchestrationTrace\" in _event[\"trace\"][\"trace\"]:\n",
        "                    _orch = _event[\"trace\"][\"trace\"][\"orchestrationTrace\"]\n",
        "\n",
        "                    if trace_level in [\"core\", \"outline\"]:\n",
        "                        if \"rationale\" in _orch:\n",
        "                            _rationale = _orch[\"rationale\"]\n",
        "                            print(colored(f\"{_rationale['text']}\", \"blue\"))\n",
        "\n",
        "                        if \"invocationInput\" in _orch:\n",
        "                            # NOTE: when agent determines invocations should happen in parallel\n",
        "                            # the trace objects for invocation input still come back one at a time.\n",
        "                            _input = _orch[\"invocationInput\"]\n",
        "                            print(_input)\n",
        "\n",
        "                            if \"actionGroupInvocationInput\" in _input:\n",
        "                                if 'function' in _input['actionGroupInvocationInput']:\n",
        "                                    tool = _input['actionGroupInvocationInput']['function']\n",
        "                                elif 'apiPath' in _input['actionGroupInvocationInput']:\n",
        "                                    tool = _input['actionGroupInvocationInput']['apiPath']\n",
        "                                else:\n",
        "                                    tool = 'undefined'\n",
        "                                if trace_level == \"outline\":\n",
        "                                    print(\n",
        "                                        colored(\n",
        "                                            f\"Using tool: {tool}\",\n",
        "                                            \"magenta\",\n",
        "                                        )\n",
        "                                    )\n",
        "                                else:\n",
        "                                    print(\n",
        "                                        colored(\n",
        "                                            f\"Using tool: {tool} with these inputs:\",\n",
        "                                            \"magenta\",\n",
        "                                        )\n",
        "                                    )\n",
        "                                    if (\n",
        "                                        len(\n",
        "                                            _input[\"actionGroupInvocationInput\"][\n",
        "                                                \"parameters\"\n",
        "                                            ]\n",
        "                                        )\n",
        "                                        == 1\n",
        "                                    ) and (\n",
        "                                        _input[\"actionGroupInvocationInput\"][\n",
        "                                            \"parameters\"\n",
        "                                        ][0][\"name\"]\n",
        "                                        == \"input_text\"\n",
        "                                    ):\n",
        "                                        print(\n",
        "                                            colored(\n",
        "                                                f\"{_input['actionGroupInvocationInput']['parameters'][0]['value']}\",\n",
        "                                                \"magenta\",\n",
        "                                            )\n",
        "                                        )\n",
        "                                    else:\n",
        "                                        print(\n",
        "                                            colored(\n",
        "                                                f\"{_input['actionGroupInvocationInput']['parameters']}\\n\",\n",
        "                                                \"magenta\",\n",
        "                                            )\n",
        "                                        )\n",
        "\n",
        "                            elif \"codeInterpreterInvocationInput\" in _input:\n",
        "                                if trace_level == \"outline\":\n",
        "                                    print(\n",
        "                                        colored(\n",
        "                                            f\"Using code interpreter\", \"magenta\"\n",
        "                                        )\n",
        "                                    )\n",
        "                                else:\n",
        "                                    console = Console()\n",
        "                                    _gen_code = _input[\n",
        "                                        \"codeInterpreterInvocationInput\"\n",
        "                                    ][\"code\"]\n",
        "                                    _code = f\"```python\\n{_gen_code}\\n```\"\n",
        "\n",
        "                                    console.print(\n",
        "                                        Markdown(f\"**Generated code**\\n{_code}\")\n",
        "                                    )\n",
        "\n",
        "                        if \"observation\" in _orch:\n",
        "                            if trace_level == \"core\":\n",
        "                                _output = _orch[\"observation\"]\n",
        "                                if \"actionGroupInvocationOutput\" in _output:\n",
        "                                    print(\n",
        "                                        colored(\n",
        "                                            f\"--tool outputs:\\n{_output['actionGroupInvocationOutput']['text'][0:_trace_truncation_lenght]}...\\n\",\n",
        "                                            \"magenta\",\n",
        "                                        )\n",
        "                                    )\n",
        "\n",
        "                                if \"agentCollaboratorInvocationOutput\" in _output:\n",
        "                                    _collab_name = _output[\n",
        "                                        \"agentCollaboratorInvocationOutput\"\n",
        "                                    ][\"agentCollaboratorName\"]\n",
        "                                    _collab_output_text = _output[\n",
        "                                        \"agentCollaboratorInvocationOutput\"\n",
        "                                    ][\"output\"][\"text\"][0:_trace_truncation_lenght]\n",
        "                                    print(\n",
        "                                        colored(\n",
        "                                            f\"\\n----sub-agent {_collab_name} output text:\\n{_collab_output_text}...\\n\",\n",
        "                                            \"magenta\",\n",
        "                                        )\n",
        "                                    )\n",
        "\n",
        "                                if \"finalResponse\" in _output:\n",
        "                                    print(\n",
        "                                        colored(\n",
        "                                            f\"Final response:\\n{_output['finalResponse']['text'][0:_trace_truncation_lenght]}...\",\n",
        "                                            \"cyan\",\n",
        "                                        )\n",
        "                                    )\n",
        "\n",
        "\n",
        "                    if \"modelInvocationOutput\" in _orch:\n",
        "                        _orch_step += 1\n",
        "                        _sub_step = 0\n",
        "                        print(colored(f\"---- Step {_orch_step} ----\", \"green\"))\n",
        "\n",
        "                        _llm_usage = _orch[\"modelInvocationOutput\"][\"metadata\"][\n",
        "                            \"usage\"\n",
        "                        ]\n",
        "                        _in_tokens = _llm_usage[\"inputTokens\"]\n",
        "                        _total_in_tokens += _in_tokens\n",
        "\n",
        "                        _out_tokens = _llm_usage[\"outputTokens\"]\n",
        "                        _total_out_tokens += _out_tokens\n",
        "\n",
        "                        _total_llm_calls += 1\n",
        "                        _orch_duration = (\n",
        "                            datetime.now() - _time_before_orchestration\n",
        "                        )\n",
        "\n",
        "                        print(\n",
        "                            colored(\n",
        "                                f\"Took {_orch_duration.total_seconds():,.1f}s, using {_in_tokens+_out_tokens} tokens (in: {_in_tokens}, out: {_out_tokens}) to complete prior action, observe, orchestrate.\",\n",
        "                                \"yellow\",\n",
        "                            )\n",
        "                        )\n",
        "\n",
        "                        # restart the clock for next step/sub-step\n",
        "                        _time_before_orchestration = datetime.now()\n",
        "\n",
        "                elif \"preProcessingTrace\" in _event[\"trace\"][\"trace\"]:\n",
        "                    _pre = _event[\"trace\"][\"trace\"][\"preProcessingTrace\"]\n",
        "                    if \"modelInvocationOutput\" in _pre:\n",
        "                        _llm_usage = _pre[\"modelInvocationOutput\"][\"metadata\"][\n",
        "                            \"usage\"\n",
        "                        ]\n",
        "                        _in_tokens = _llm_usage[\"inputTokens\"]\n",
        "                        _total_in_tokens += _in_tokens\n",
        "\n",
        "                        _out_tokens = _llm_usage[\"outputTokens\"]\n",
        "                        _total_out_tokens += _out_tokens\n",
        "\n",
        "                        _total_llm_calls += 1\n",
        "\n",
        "                        print(\n",
        "                            colored(\n",
        "                                \"Pre-processing trace, agent came up with an initial plan.\",\n",
        "                                \"yellow\",\n",
        "                            )\n",
        "                        )\n",
        "                        print(\n",
        "                            colored(\n",
        "                                f\"Used LLM tokens, in: {_in_tokens}, out: {_out_tokens}\",\n",
        "                                \"yellow\",\n",
        "                            )\n",
        "                        )\n",
        "\n",
        "                elif \"postProcessingTrace\" in _event[\"trace\"][\"trace\"]:\n",
        "                    _post = _event[\"trace\"][\"trace\"][\"postProcessingTrace\"]\n",
        "                    if \"modelInvocationOutput\" in _post:\n",
        "                        _llm_usage = _post[\"modelInvocationOutput\"][\"metadata\"][\n",
        "                            \"usage\"\n",
        "                        ]\n",
        "                        _in_tokens = _llm_usage[\"inputTokens\"]\n",
        "                        _total_in_tokens += _in_tokens\n",
        "\n",
        "                        _out_tokens = _llm_usage[\"outputTokens\"]\n",
        "                        _total_out_tokens += _out_tokens\n",
        "\n",
        "                        _total_llm_calls += 1\n",
        "                        print(colored(\"Agent post-processing complete.\", \"yellow\"))\n",
        "                        print(\n",
        "                            colored(\n",
        "                                f\"Used LLM tokens, in: {_in_tokens}, out: {_out_tokens}\",\n",
        "                                \"yellow\",\n",
        "                            )\n",
        "                        )\n",
        "\n",
        "                if trace_level == \"all\":\n",
        "                    print(json.dumps(_event[\"trace\"], indent=2))\n",
        "\n",
        "            if \"files\" in _event.keys() and request_params[\"enableTrace\"]:\n",
        "                console = Console()\n",
        "                files_event = _event[\"files\"]\n",
        "                console.print(Markdown(\"**Files**\"))\n",
        "\n",
        "                files_list = files_event[\"files\"]\n",
        "                for this_file in files_list:\n",
        "                    print(f\"{this_file['name']} ({this_file['type']})\")\n",
        "                    file_bytes = this_file[\"bytes\"]\n",
        "\n",
        "                    # save bytes to file, given the name of file and the bytes\n",
        "                    file_name = os.path.join(\"output\", this_file[\"name\"])\n",
        "                    with open(file_name, \"wb\") as f:\n",
        "                        f.write(file_bytes)\n",
        "\n",
        "        if request_params[\"enableTrace\"]:\n",
        "            duration = datetime.now() - _time_before_call\n",
        "\n",
        "            if trace_level in [\"core\", \"outline\"]:\n",
        "                print(\n",
        "                    colored(\n",
        "                        f\"Agent made a total of {_total_llm_calls} LLM calls, \"\n",
        "                        + f\"using {_total_in_tokens+_total_out_tokens} tokens \"\n",
        "                        + f\"(in: {_total_in_tokens}, out: {_total_out_tokens})\"\n",
        "                        + f\", and took {duration.total_seconds():,.1f} total seconds\",\n",
        "                        \"yellow\",\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            if trace_level == \"all\":\n",
        "                print(f\"Returning agent answer as: {_agent_answer}\")\n",
        "\n",
        "        return _agent_answer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Caught exception while processing input to invokeAgent:\\n\")\n",
        "        input_text = request_params[\"inputText\"]\n",
        "        print(f\"  for input text:\\n{input_text}\\n\")\n",
        "        print(\n",
        "            f\"  request ID: {_agent_resp['ResponseMetadata']['RequestId']}, retries: {_agent_resp['ResponseMetadata']['RetryAttempts']}\\n\"\n",
        "        )\n",
        "        print(f\"Error: {e}\")\n",
        "        raise Exception(\"Unexpected exception: \", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "tags": [],
        "id": "Woy-D8Ne7sRG",
        "outputId": "03576608-ddf0-46c0-8444-2dbc54d44eb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "invokeAgent API request ID: 92a1719e-15bb-406c-a471-1f76c79faef2\n",
            "invokeAgent API session ID: custom-session-id-18266\n",
            "Agent error: Invocation of model ID anthropic.claude-3-7-sonnet-20250219-v1:0 with on-demand throughput isn’t supported. Retry your request with the ID or ARN of an inference profile that contains this model.\n",
            "Caught exception while processing input to invokeAgent:\n",
            "\n",
            "  for input text:\n",
            "what is the time right now in pacific timezone?\n",
            "\n",
            "  request ID: 92a1719e-15bb-406c-a471-1f76c79faef2, retries: 0\n",
            "\n",
            "Error: An error occurred (validationException) when calling the InvokeInlineAgent operation: Invocation of model ID anthropic.claude-3-7-sonnet-20250219-v1:0 with on-demand throughput isn’t supported. Retry your request with the ID or ARN of an inference profile that contains this model.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "('Unexpected exception: ', EventStreamError('An error occurred (validationException) when calling the InvokeInlineAgent operation: Invocation of model ID anthropic.claude-3-7-sonnet-20250219-v1:0 with on-demand throughput isn’t supported. Retry your request with the ID or ARN of an inference profile that contains this model.'))",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEventStreamError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-e3b040ca9318>\u001b[0m in \u001b[0;36minvoke_inline_agent_helper\u001b[0;34m(client, request_params, trace_level)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_event\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_event_stream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0m_sub_agent_alias_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/botocore/eventstream.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m             \u001b[0mparsed_event\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparsed_event\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/botocore/eventstream.py\u001b[0m in \u001b[0;36m_parse_event\u001b[0;34m(self, event)\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEventStreamError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mEventStreamError\u001b[0m: An error occurred (validationException) when calling the InvokeInlineAgent operation: Invocation of model ID anthropic.claude-3-7-sonnet-20250219-v1:0 with on-demand throughput isn’t supported. Retry your request with the ID or ARN of an inference profile that contains this model.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-62488b05f348>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minvoke_inline_agent_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbedrock_rt_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"core\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-e3b040ca9318>\u001b[0m in \u001b[0;36minvoke_inline_agent_helper\u001b[0;34m(client, request_params, trace_level)\u001b[0m\n\u001b[1;32m    287\u001b[0m         )\n\u001b[1;32m    288\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unexpected exception: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mException\u001b[0m: ('Unexpected exception: ', EventStreamError('An error occurred (validationException) when calling the InvokeInlineAgent operation: Invocation of model ID anthropic.claude-3-7-sonnet-20250219-v1:0 with on-demand throughput isn’t supported. Retry your request with the ID or ARN of an inference profile that contains this model.'))"
          ]
        }
      ],
      "source": [
        "invoke_inline_agent_helper(bedrock_rt_client, request_params, trace_level=\"core\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJFKO-3L7sRG"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook has demonstrated the key aspects of using the Amazon Bedrock Inline Agents API:\n",
        "\n",
        "1. Basic agent invocation\n",
        "\n",
        "For more advance topics such as the following, please take a look at the deep dive [agents workshop](https://catalog.workshops.aws/agents-for-amazon-bedrock/en-US):\n",
        "\n",
        "2. Incorporating knowledge bases\n",
        "3. Adding custom action groups\n",
        "4. Implementing guardrails\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "birDxr1X7sRG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "availableInstances": [
      {
        "_defaultOrder": 0,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.t3.medium",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 1,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.t3.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 2,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.t3.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 3,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.t3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 4,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 5,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 6,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 7,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 8,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 9,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 10,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 11,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 12,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5d.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 13,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5d.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 14,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5d.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 15,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5d.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 16,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5d.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 17,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5d.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 18,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5d.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 19,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 20,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": true,
        "memoryGiB": 0,
        "name": "ml.geospatial.interactive",
        "supportedImageNames": [
          "sagemaker-geospatial-v1-0"
        ],
        "vcpuNum": 0
      },
      {
        "_defaultOrder": 21,
        "_isFastLaunch": true,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.c5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 22,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.c5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 23,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.c5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 24,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.c5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 25,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 72,
        "name": "ml.c5.9xlarge",
        "vcpuNum": 36
      },
      {
        "_defaultOrder": 26,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 96,
        "name": "ml.c5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 27,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 144,
        "name": "ml.c5.18xlarge",
        "vcpuNum": 72
      },
      {
        "_defaultOrder": 28,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.c5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 29,
        "_isFastLaunch": true,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g4dn.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 30,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g4dn.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 31,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g4dn.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 32,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g4dn.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 33,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g4dn.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 34,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g4dn.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 35,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 61,
        "name": "ml.p3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 36,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 244,
        "name": "ml.p3.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 37,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 488,
        "name": "ml.p3.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 38,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.p3dn.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 39,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.r5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 40,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.r5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 41,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.r5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 42,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.r5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 43,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.r5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 44,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.r5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 45,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.r5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 46,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.r5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 47,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 48,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 49,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 50,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 51,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 52,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 53,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.g5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 54,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.g5.48xlarge",
        "vcpuNum": 192
      },
      {
        "_defaultOrder": 55,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 56,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4de.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 57,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.trn1.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 58,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.trn1.32xlarge",
        "vcpuNum": 128
      },
      {
        "_defaultOrder": 59,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.trn1n.32xlarge",
        "vcpuNum": 128
      }
    ],
    "instance_type": "ml.t3.medium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}